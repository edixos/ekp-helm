# Default values for alertmanager.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# nameOverride --
nameOverride: ""
# fullnameOverride --
fullnameOverride: ""

# -- Labels to apply to all resources
commonLabels: {}

# -- Reference to one or more secrets to be used when pulling images
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# - name: "image-pull-secret"

image:
# image.repository -- Override alertmanager image name, use image defined by alertmanager controller if not defined
  repository: ""
# image.tag -- Override image tag, use `.Chart.appVersion` by default
  tag: ""


global:
# -- Annotate Custom Resources with `global.argocdAnnotations`
  enableArgocdAnnotations: false
  argocdAnnotations:
    - "argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true"


# -- Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
# running cluster equal to the expected size.
replicaCount: 3

# clusterName -- Alertmanager name
clusterName: ""

# -- Standard objectâ€™s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
# Metadata Labels and Annotations gets propagated to the Alertmanager pods.
podMetadata: {}

# -- If true then the user will be responsible to provide a secret with alertmanager configuration
# So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
# See `configSecret`
useExistingSecret: false

# -- List of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the
# Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.
secrets: []

# -- Name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for
# this Alertmanager instance.
# The secret is mounted into /etc/alertmanager/config and should contains `alertmanager.yaml` key.
# @default -- 'alertmanager-{{ fullname }}'
configSecret: ""

# -- List of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
# The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
configMaps: []

# -- Define Log Format
# Use logfmt (default) or json-formatted logging
logFormat: logfmt

# -- Log level for Alertmanager to be configured with.
logLevel: info

# -- Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
# [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
retention: 120h

# -- Storage is the definition of how storage will be used by the Alertmanager instances.
# ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md
storage: {}
# volumeClaimTemplate:
#   spec:
#     storageClassName: gluster
#     accessModes: ["ReadWriteOnce"]
#     resources:
#       requests:
#         storage: 50Gi
#   selector: {}

# -- Additional volumes on the output StatefulSet definition.
volumes: []

# -- Additional VolumeMounts on the output StatefulSet definition.
volumeMounts: []

# -- The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs.
# This is necessary if Alertmanager is not served from root of a DNS name.
# @default -- Use ingress host if enabled else generated from Chart name
externalUrl: ""

# -- The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy
# is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
# but the server serves requests under a different route prefix. For example for use with kubectl proxy.
routePrefix: /

# -- If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
paused: false

# -- Define which Nodes the Pods are scheduled on.
# ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

# Define resources requests and limits for single Pods.
# ref: https://kubernetes.io/docs/user-guide/compute-resources/
resources:
  requests:
    cpu: 5m
    memory: 200Mi
  limits:
    cpu: 100m
    memory: 500Mi

# -- Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
# The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
# The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
# The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
podAntiAffinity: ""

# -- If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
# This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
podAntiAffinityTopologyKey: kubernetes.io/hostname

# -- Assign custom affinity rules to the alertmanager instance
# ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
affinity: {}
# nodeAffinity:
#   requiredDuringSchedulingIgnoredDuringExecution:
#     nodeSelectorTerms:
#     - matchExpressions:
#       - key: kubernetes.io/e2e-az-name
#         operator: In
#         values:
#         - e2e-az1
#         - e2e-az2

# -- If specified, the pod's tolerations.
# ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []
# - key: "key"
#   operator: "Equal"
#   value: "value"
#   effect: "NoSchedule"

# -- SecurityContext holds pod-level security attributes and common container settings.
# This defaults to non root user with uid 1000 and gid 2000.	*v1.PodSecurityContext	false
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
securityContext:
  runAsGroup: 2000
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 2000

# -- ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.
# Note this is only for the Alertmanager UI, not the gossip communication.
listenLocal: false

# -- Allows to inject additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.
containers: []

# -- Priority class assigned to the Pods
priorityClassName: ""

# -- AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.
additionalPeers: []

# -- PortName to use for Alert Manager.
portName: "web"

# -- Explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses.
# [1] RFC1918: https://tools.ietf.org/html/rfc1918
clusterAdvertiseAddress: false


# Configuration for AlertManager service
service:
  # -- Map of labels to apply to the service
  labels: {}

  # -- Map of annotations to apply to the service
  annotations: {}

  # -- Cluster IP
  # Only use if service.type is "ClusterIP"
  clusterIP: ""

  # -- Port for Prometheus Service to listen on
  port: 9093

  # -- To be used with a proxy extraContainer port
  targetPort: 9093

  # -- List of IP addresses at which the Prometheus server service is available
  # Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  externalIPs: []

  # -- Port to expose on each node
  # Only used if service.type is 'NodePort'
  nodePort: 30090

  # -- Loadbalancer IP
  # Only use if service.type is "loadbalancer"
  loadBalancerIP: ""

  # service.loadSourceRanges -- Only use if service.type is "loadbalancer"
  loadBalancerSourceRanges: []

  # -- Service type
  type: ClusterIP

  # service.sessionAffinity --
  sessionAffinity: ""


istio:
  # -- Enables istio features
  enabled: false


ingress:
  # -- Enables ingress for alertmanager
  enabled: false

  # -- Map of labels to apply to the ingress
  labels: {}

  #  -- Map of default annotations to apply to the ingress
  annotations:
    kubernetes.io/ingress.allow-http: "false"

  acme:
    # -- Manage certificates with ACME protocol
    enabled: true
    # -- Annotations to add when `ingress.acme.enabled` is true
    annotations:
      - 'kubernetes.io/tls-acme: "true"'

  # -- Map of annotations to add  to th ingress
  extraAnnotations: {}

  # -- FQDN of the prometheus
  host: ""

  # -- Path of the incoming request (/* or / if used with nginx)
  path: "/*"

  tls:
    # -- Name of the secret containing the certificates
    # @default -- Generated name based on chart release full name
    secretName: ""


prometheus:
  # -- Enables prometheus operator resources
  enabled: true
  serviceMonitor:
    # -- Enables prometheus operator service monitor
    enabled: true
    # -- Map of labels to apply to the servicemonitor
    labels:
      prometheus: prometheus-operator-prometheus
    # -- Scrape interval. If not set, the Prometheus default scrape interval is used. dd
    interval: ""
    # -- HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
    scheme: ""
    # -- TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
    # Of type: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
    tlsConfig: {}
    # --
    bearerTokenFile:

    # -- metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # -- Relabel configs to apply to samples before ingestion.
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    # -- Map of annotations to apply to the ServiceMonitor
    annotations: {}


  rules:
    # -- Labels to affect to the Prometheus Rules
    labels:
      prometheus: prometheus-operator-prometheus

  grafanaDashboard:
    # -- If `true`, deploy grafana dashboard
    enabled: true
    # -- Labels to apply to dashboard configmap
    label:
      grafana_dashboard: "1"

# -- Configure pod disruption budgets for AlertManager
# ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
# This configuration is immutable once created and will require the PDB to be deleted to be changed
# https://github.com/kubernetes/kubernetes/issues/45398
podDisruptionBudget:
  enabled: true
  minAvailable: 1
  maxUnavailable: ""
# Service account for AlertManager to use.
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
serviceAccount:
  # -- When `true`, create alertmanager serviceAccount with `serviceAccount.name`.Â If `serviceAccount.name` is empty, use `.Chart.fullname` value.
  create: true
  # -- Name of the ServiceAccount to use to run the Prometheus Pods. If `prometheus.serviceAccount.create` is `false` and no name is defined, use `default`.
  name: ""
  # -- Annotations for ServiceAccount
  annotations: {}
  # --          automountServiceAccountToken
  automountServiceAccountToken: false


oidc:
  global: {}
  # To help compatibility with other charts which use global.imagePullSecrets.
  # global:
  #   imagePullSecrets:
  #   - name: pullSecret1
  #   - name: pullSecret2
  
  ## Override the deployment namespace
  ##
  namespaceOverride: ""
  
  # Force the target Kubernetes version (it uses Helm `.Capabilities` if not set).
  # This is especially useful for `helm template` as capabilities are always empty
  # due to the fact that it doesn't query an actual cluster
  kubeVersion:
  
  # Oauth client configuration specifics
  config:
    # Add config annotations
    annotations: {}
    # OAuth client ID
    clientID: "XXXXXXX"
    # OAuth client secret
    clientSecret: "XXXXXXXX"
    # Create a new secret with the following command
    # openssl rand -base64 32 | head -c 32 | base64
    # Use an existing secret for OAuth2 credentials (see secret.yaml for required fields)
    # Example:
    # existingSecret: secret
    cookieSecret: "XXXXXXXXXXXXXXXX"
    # The name of the cookie that oauth2-proxy will create
    # If left empty, it will default to the release name
    cookieName: ""
    google: {}
      # adminEmail: xxxx
      # useApplicationDefaultCredentials: true
      # targetPrincipal: xxxx
      # serviceAccountJson: xxxx
      # Alternatively, use an existing secret (see google-secret.yaml for required fields)
      # Example:
      # existingSecret: google-secret
      # groups: []
      # Example:
      #  - group1@example.com
      #  - group2@example.com
    # Default configuration, to be overridden
    configFile: |-
      email_domains = [ "*" ]
      upstreams = [ "file:///dev/null" ]
    # Custom configuration file: oauth2_proxy.cfg
    # configFile: |-
    #   pass_basic_auth = false
    #   pass_access_token = true
    # Use an existing config map (see configmap.yaml for required fields)
    # Example:
    # existingConfig: config
  
  alphaConfig:
    enabled: false
    # Add config annotations
    annotations: {}
    # Arbitrary configuration data to append to the server section
    serverConfigData: {}
    # Arbitrary configuration data to append to the metrics section
    metricsConfigData: {}
    # Arbitrary configuration data to append
    configData: {}
    # Arbitrary configuration to append
    # This is treated as a Go template and rendered with the root context
    configFile: ""
    # Use an existing config map (see secret-alpha.yaml for required fields)
    existingConfig: ~
    # Use an existing secret
    existingSecret: ~
  
  image:
    repository: "quay.io/oauth2-proxy/oauth2-proxy"
    # appVersion is used by default
    tag: ""
    pullPolicy: "IfNotPresent"
    command: []
  
  # Optionally specify an array of imagePullSecrets.
  # Secrets must be manually created in the namespace.
  # ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  imagePullSecrets: []
    # - name: myRegistryKeySecretName
  
  # Set a custom containerPort if required.
  # This will default to 4180 if this value is not set and the httpScheme set to http
  # This will default to 4443 if this value is not set and the httpScheme set to https
  # containerPort: 4180
  
  extraArgs: {}
  extraEnv: []
  
  envFrom: []
  # Load environment variables from a ConfigMap(s) and/or Secret(s)
  # that already exists (created and managed by you).
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables
  #
  # PS: Changes in these ConfigMaps or Secrets will not be automatically
  #     detected and you must manually restart the relevant Pods after changes.
  #
  #  - configMapRef:
  #      name: special-config
  #  - secretRef:
  #      name: special-config-secret
  
  # -- Custom labels to add into metadata
  customLabels: {}
  
  # To authorize individual email addresses
  # That is part of extraArgs but since this needs special treatment we need to do a separate section
  authenticatedEmailsFile:
    enabled: false
    # Defines how the email addresses file will be projected, via a configmap or secret
    persistence: configmap
    # template is the name of the configmap what contains the email user list but has been configured without this chart.
    # It's a simpler way to maintain only one configmap (user list) instead changing it for each oauth2-proxy service.
    # Be aware the value name in the extern config map in data needs to be named to "restricted_user_access" or to the
    # provided value in restrictedUserAccessKey field.
    template: ""
    # The configmap/secret key under which the list of email access is stored
    # Defaults to "restricted_user_access" if not filled-in, but can be overridden to allow flexibility
    restrictedUserAccessKey: ""
    # One email per line
    # example:
    # restricted_access: |-
    #   name1@domain
    #   name2@domain
    # If you override the config with restricted_access it will configure a user list within this chart what takes care of the
    # config map resource.
    restricted_access: ""
    annotations: {}
    # helm.sh/resource-policy: keep
  
  service:
    type: ClusterIP
    # when service.type is ClusterIP ...
    # clusterIP: 192.0.2.20
    # when service.type is LoadBalancer ...
    # loadBalancerIP: 198.51.100.40
    # loadBalancerSourceRanges: 203.0.113.0/24
    # when service.type is NodePort ...
    # nodePort: 80
    portNumber: 80
    # Protocol set on the service
    appProtocol: http
    annotations: {}
    # foo.io/bar: "true"
    # configure externalTrafficPolicy
    externalTrafficPolicy: ""
    # configure internalTrafficPolicy
    internalTrafficPolicy: ""
    # configure service target port
    targetPort: ""
  
  ## Create or use ServiceAccount
  serviceAccount:
    ## Specifies whether a ServiceAccount should be created
    enabled: true
    ## The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the fullname template
    name:
    automountServiceAccountToken: true
    annotations: {}
  
  ingress:
    enabled: false
    # className: nginx
    path: /
    # Only used if API capabilities (networking.k8s.io/v1) allow it
    pathType: ImplementationSpecific
    # Used to create an Ingress record.
    # hosts:
      # - chart-example.local
    # Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    # Warning! The configuration is dependant on your current k8s API version capabilities (networking.k8s.io/v1)
    # extraPaths:
    # - path: /*
    #   pathType: ImplementationSpecific
    #   backend:
    #     service:
    #       name: ssl-redirect
    #       port:
    #         name: use-annotation
    labels: {}
    # annotations:
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: "true"
    # tls:
      # Secrets must be manually created in the namespace.
      # - secretName: chart-example-tls
      #   hosts:
      #     - chart-example.local
  
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 300Mi
    # requests:
    #   cpu: 100m
    #   memory: 300Mi
  
  extraVolumes: []
    # - name: ca-bundle-cert
    #   secret:
    #     secretName: <secret-name>
  
  extraVolumeMounts: []
    # - mountPath: /etc/ssl/certs/
    #   name: ca-bundle-cert
  
  # Additional containers to be added to the pod.
  extraContainers: []
    #  - name: my-sidecar
    #    image: nginx:latest
  
  # Additional Init containers to be added to the pod.
  extraInitContainers: []
    #  - name: wait-for-idp
    #    image: my-idp-wait:latest
    #    command:
    #    - sh
    #    - -c
    #    - wait-for-idp.sh
  
  priorityClassName: ""
  
  # hostAliases is a list of aliases to be added to /etc/hosts for network name resolution
  hostAliases: []
  # - ip: "10.xxx.xxx.xxx"
  #   hostnames:
  #     - "auth.example.com"
  # - ip: 127.0.0.1
  #   hostnames:
  #     - chart-example.local
  #     - example.local
  
  # [TopologySpreadConstraints](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/) configuration.
  # Ref: https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling
  # topologySpreadConstraints: []
  
  # Affinity for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  # affinity: {}
  
  # Tolerations for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []
  
  # Node labels for pod assignment
  # Ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}
  
  # Whether to use secrets instead of environment values for setting up OAUTH2_PROXY variables
  proxyVarsAsSecrets: true
  
  # Configure Kubernetes liveness and readiness probes.
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
  # Disable both when deploying with Istio 1.0 mTLS. https://istio.io/help/faq/security/#k8s-health-checks
  livenessProbe:
    enabled: true
    initialDelaySeconds: 0
    timeoutSeconds: 1
  
  readinessProbe:
    enabled: true
    initialDelaySeconds: 0
    timeoutSeconds: 5
    periodSeconds: 10
    successThreshold: 1
  
  # Configure Kubernetes security context for container
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  securityContext:
    enabled: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 2000
    runAsGroup: 2000
    seccompProfile:
      type: RuntimeDefault
  
  deploymentAnnotations: {}
  podAnnotations: {}
  podLabels: {}
  replicaCount: 1
  revisionHistoryLimit: 10
  strategy: {}
  enableServiceLinks: true
  
  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  
  ## Horizontal Pod Autoscaling
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
  #  targetMemoryUtilizationPercentage: 80
    annotations: {}
  
  # Configure Kubernetes security context for pod
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  podSecurityContext: {}
  
  # whether to use http or https
  httpScheme: http
  
  initContainers:
    # if the redis sub-chart is enabled, wait for it to be ready
    # before starting the proxy
    # creates a role binding to get, list, watch, the redis master pod
    # if service account is enabled
    waitForRedis:
      enabled: true
      image:
        repository: "alpine"
        tag: "latest"
        pullPolicy: "IfNotPresent"
      # uses the kubernetes version of the cluster
      # the chart is deployed on, if not set
      kubectlVersion: ""
      securityContext:
        enabled: true
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      timeout: 180
      resources: {}
        # limits:
        #   cpu: 100m
        #   memory: 300Mi
        # requests:
        #   cpu: 100m
        #   memory: 300Mi
  
  # Additionally authenticate against a htpasswd file. Entries must be created with "htpasswd -B" for bcrypt encryption.
  # Alternatively supply an existing secret which contains the required information.
  htpasswdFile:
    enabled: false
    existingSecret: ""
    entries: []
    # One row for each user
    # example:
    # entries:
    #  - testuser:$2y$05$gY6dgXqjuzFhwdhsiFe7seM9q9Tile4Y3E.CBpAZJffkeiLaC21Gy
  
  # Configure the session storage type, between cookie and redis
  sessionStorage:
    # Can be one of the supported session storage cookie|redis
    type: cookie
    redis:
      # Name of the Kubernetes secret containing the redis & redis sentinel password values (see also `sessionStorage.redis.passwordKey`)
      existingSecret: ""
      # Redis password value. Applicable for all Redis configurations. Taken from redis subchart secret if not set. `sessionStorage.redis.existingSecret` takes precedence
      password: ""
      # Key of the Kubernetes secret data containing the redis password value. If you use the redis sub chart, make sure
      # this password matches the one used in redis.global.redis.password (see below).
      passwordKey: "redis-password"
      # Can be one of standalone|cluster|sentinel
      clientType: "standalone"
      standalone:
        # URL of redis standalone server for redis session storage (e.g. `redis://HOST[:PORT]`). Automatically generated if not set
        connectionUrl: ""
      cluster:
        # List of Redis cluster connection URLs. Array or single string allowed.
        connectionUrls: []
        # - "redis://127.0.0.1:8000"
        # - "redis://127.0.0.1:8001"
      sentinel:
        # Name of the Kubernetes secret containing the redis sentinel password value (see also `sessionStorage.redis.sentinel.passwordKey`). Default: `sessionStorage.redis.existingSecret`
        existingSecret: ""
        # Redis sentinel password. Used only for sentinel connection; any redis node passwords need to use `sessionStorage.redis.password`
        password: ""
        # Key of the Kubernetes secret data containing the redis sentinel password value
        passwordKey: "redis-sentinel-password"
        # Redis sentinel master name
        masterName: ""
        # List of Redis cluster connection URLs. Array or single string allowed.
        connectionUrls: []
        # - "redis://127.0.0.1:8000"
        # - "redis://127.0.0.1:8001"
  
  # Enables and configure the automatic deployment of the redis subchart
  redis:
    # provision an instance of the redis sub-chart
    enabled: false
    # Redis specific helm chart settings, please see:
    # https://github.com/bitnami/charts/tree/master/bitnami/redis#parameters
    # global:
    #   redis:
    #     password: yourpassword
    # If you install Redis using this sub chart, make sure that the password of the sub chart matches the password
    # you set in sessionStorage.redis.password (see above).
    # redisPort: 6379
    # architecture: standalone
  
  # Enables apiVersion deprecation checks
  checkDeprecation: true
  
  # Allows graceful shutdown
  # terminationGracePeriodSeconds: 65
  # lifecycle:
  #   preStop:
  #     exec:
  #       command: [ "sh", "-c", "sleep 60" ]
  
  metrics:
    # Enable Prometheus metrics endpoint
    enabled: true
    # Serve Prometheus metrics on this port
    port: 44180
    # when service.type is NodePort ...
    # nodePort: 44180
    # Protocol set on the service for the metrics port
    service:
      appProtocol: http
    serviceMonitor:
      # Enable Prometheus Operator ServiceMonitor
      enabled: false
      # Define the namespace where to deploy the ServiceMonitor resource
      namespace: ""
      # Prometheus Instance definition
      prometheusInstance: default
      # Prometheus scrape interval
      interval: 60s
      # Prometheus scrape timeout
      scrapeTimeout: 30s
      # Add custom labels to the ServiceMonitor resource
      labels: {}
  
      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""
  
      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
      tlsConfig: {}
  
      ## bearerTokenFile: Path to bearer token file.
      bearerTokenFile: ""
  
      ## Used to pass annotations that are used by the Prometheus installed in your cluster to select Service Monitors to work with
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      annotations: {}
  
      ## Metric relabel configs to apply to samples before ingestion.
      ## [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]
  
      ## Relabel configs to apply to samples before ingestion.
      ## [Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config)
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
  
  # Extra K8s manifests to deploy
  extraObjects: []
    # - apiVersion: secrets-store.csi.x-k8s.io/v1
    #   kind: SecretProviderClass
    #   metadata:
    #     name: oauth2-proxy-secrets-store
    #   spec:
    #     provider: aws
    #     parameters:
    #       objects: |
    #         - objectName: "oauth2-proxy"
    #           objectType: "secretsmanager"
    #           jmesPath:
    #               - path: "client_id"
    #                 objectAlias: "client-id"
    #               - path: "client_secret"
    #                 objectAlias: "client-secret"
    #               - path: "cookie_secret"
    #                 objectAlias: "cookie-secret"
    #     secretObjects:
    #     - data:
    #       - key: client-id
    #         objectName: client-id
    #         - key: client-secret
    #           objectName: client-secret
    #         - key: cookie-secret
    #         objectName: cookie-secret
    #       secretName: oauth2-proxy-secrets-store
    #       type: Opaque
  # -- If `true`, enable oidc authentification with sidecar container
  enabled: false

  # Image of oauth2-proxy. Manage image with operator if not defined
  image:
    # -- Container name for oauth2-proxy sidecar
    repository: quay.io/oauth2-proxy/oauth2-proxy
    # -- Container image pull policy for oauth2-proxy sidecar
    pullPolicy: IfNotPresent

  # -- PortName to use for oidc proxy sidecar
  portName: http-oidc
  # -- Port to listen by oidc proxy
  port: 3000

  # -- Deploy prometheus ServiceMonitor resource to scrape oidc proxy metrics.
  serviceMonitor: true
  # -- PortName to use for oidc proxy sidecar metrics
  metricsPortName: http-oauth-prom
  # -- Port number where to expose prometheus proxy for oidc proxy
  metricsPort: 3090

  # -- Environment variables to inject into sidecar
  env: []
  # - name: "http_proxy"
  #   value: "{{ .Values.httpProxy }}"
  # - name: "https_proxy"
  #   value: "{{ .Values.httpProxy }}"
  # - name: "no_proxy"
  #   value: "{{ .Values.noProxy }},*.cluster.local"

  configMap:
    # -- Create and configure configmap  with name `oidc.configMap.name`
    create: false
    # -- Configmap name to inject into sidecar
    # @default -- Generated from release chart name
    name: ""
    # -- Map of annotations to apply to the configMap
    annotations: {}
    # -- Required, the OpenID Connect issuer URL, e.g. "https://accounts.google.com"
    issuerUrl: ""
    # -- Upstream service to proxy
    # @default -- http://localhost:<service.targetPort>
    upstreamUrl: ""
    # -- Override the provider's name with the given string; used for the sign-in page
    providerDisplayName: ""
    # -- Authenticate emails with the specified domain. Use * to authenticate any email
    emailDomains:
      - "*"

    # -- Are we running behind a reverse proxy, controls whether headers like X-Real-IP are accepted and allows
    # X-Forwarded-{Proto,Host,Uri} headers to be used on redirect selection
    reverseProxy: true

    # -- Pass HTTP Basic Auth, X-Forwarded-User and X-Forwarded-Email information to upstream
    passBasicAuth: true

    # -- Pass X-Forwarded-User, X-Forwarded-Groups, X-Forwarded-Email and X-Forwarded-Preferred-Username information to upstream
    passUserHeaders: true

    # -- Pass OAuth Access token to upstream via "X-Forwarded-Access-Token"
    passAccessToken: true

    # -- Set X-Auth-Request-User, X-Auth-Request-Groups, X-Auth-Request-Email and X-Auth-Request-Preferred-Username.
    # When used with pass_access_token, X-Auth-Request-Access-Token is added to response headers
    setXauthrequest: false

    # -- the cookie name for use with an AES cipher when cookie_refresh or pass_access_token is set
    cookieName: "_oauth2_proxy"
    # -- Cookie domain to force cookies to (ie: .yourcompany.com)
    cookieDomains: ""
    # -- Expire timeframe for cookie
    cookieExpire: "12h"
    # -- Refresh the cookie when duration has elapsed after cookie was initially set.
    # Should be less than cookie_expire; set to 0 to disable.
    # On refresh, OAuth token is re-validated.
    # (ie: 1h means tokens are refreshed on request 1hr+ after it was set)
    cookieRefresh: ""
    # -- Secure cookies are only sent by the browser of a HTTPS connection (recommended)
    cookieSecure: true
    # -- HttpOnly cookies are not readable by javascript (recommended)
    cookieHttponly: true

    # -- Extra options to add to configuration file.
    # See [oauth2-proxy documentation](https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/overview/#config-file)
    # for details
    extraConfig: ""
    # silence_ping_logging = true
  secret:
    # -- Create and configure secrets with name `oidc.secret.name`. If `false`, use existing secret.
    create: true
    # -- Secret name use to store oidc secrets
    # @default -- Generated from release chart name
    name: ""
    # -- Map of annotations to apply to the Secret created
    annotations: {}
    # -- Secret key name for encryption key.
    # If `oidc.secret.create` is `true`, a secret with this key will be generated.
    # Else, this key matches existing key in `oidc.secret.name`.
    # The value key length should be either 16 or 32 bytes, depending or whether you want AES-128 or AES-256
    cookieSecretKey: "encryption_key"
    # -- Secret key name for OAUTH2 client secret.
    # If `oidc.secret.create` is `true`, a secret with this key will be generated.
    # Else, this key matches existing key in `oidc.secret.name`.
    clientSecretKey: "client_secret"

  # -- OAUTH2 client id
  # @default -- release chart full name
  applicationId: ""

  dexClient:
    # -- Manage aplicationId/secret as Dex resource
    enabled: false
    # -- Map of annotations to apply to the dex Client created
    annotations: {}

  # -- Add resources limits and request to oidc proxy side-car container.
  resources:
    limits:
      cpu: 100m
      memory: 50Mi
    requests:
      cpu: 5m
      memory: 30Mi


authorizations:
  # -- Enable `envoy-proxy` sidecar to manage rbac access
  enabled: false

  envoy:
    port: 8888

    image:
      # -- Container name for envoy-proxy
      repository: docker.io/envoyproxy/envoy-distroless
      # image.tag -- Container image tag
      tag: v1.22-latest
      # -- Container image pull policy
      pullPolicy: Always

    # -- Add resources limits and request to envoy proxy side-car container.
    resources:
      limits:
        cpu: 100m
        memory: 50Mi
      requests:
        cpu: 5m
        memory: 30Mi

    configMap:
      # -- Create and configure configmap  with name `authorizations.envoy.configMap.name`
      create: true
      # -- Configmap name to inject into envoy sidecar
      # @default -- Generated from release chart name
      name: ""

  rbac:
    # -- List of resources to protect (name: <rule_name>  ,uri: /<URI>, methods: [<METHOD>], roles: [])
    resources: {}
      # - name: dev
      #   uri: /*
      #   methods:
      #     - GET
      #     - POST
      #   groups:
      #    - developer

karma:
  # -- Enable karma authentication with htpasswd file
  enabled: false
  htpasswd:
    # -- Secret name that contains htpasswd file used to share credentials with karma instance
    secretName: karma-secret
    # -- Secret key that contains htpasswd file used to share credentials with karma instance
    secretKey: htpasswd
  # -- User groups set when authenticated with htpasswd file
  groups:
    - karma
