# -- Provide a name in place of prometheus for `app:` labels
nameOverride: ""

# -- Provide a name to substitute for the full names of resources
fullnameOverride: ""

# -- Labels to apply to all resources
commonLabels: {}

rbac:
  # -- Create rbac resources when set to `true`
  create: true
  # -- Use `Role` and `RoleBinding` resources when set to true, else `ClusterRole` and `ClusterRoleBinding` 
  scopeNamespaced: true

# -- Reference to one or more secrets to be used when pulling images
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# - name: "image-pull-secret"

alerting:
  # -- AlertmanagerEndpoints Prometheus should fire alerts against
  endpoints: []
  #  - name: alertmanager-operated
  #    namespace: ""
  #    port: web

istio:
  # -- Allow to scrap metrics using istio certificates
  useCertificates: false
  # -- Annotate prometheus pod to allow to inject side car
  podAnnotations:
    traffic.sidecar.istio.io/includeInboundPorts: ""  # do not intercept any inbound ports
    traffic.sidecar.istio.io/includeOutboundIPRanges: ""  # do not intercept any outbound traffic
    proxy.istio.io/config: |  # configure an env variable `OUTPUT_CERTS` to write certificates to the given folder
      proxyMetadata:
        OUTPUT_CERTS: /etc/istio-certs/
    sidecar.istio.io/userVolume: '[{"name": "istio-certs", "emptyDir": {"medium": "Memory"}}]'
    sidecar.istio.io/userVolumeMount: '[{"name": "istio-certs", "mountPath": "/etc/istio-certs/"}]'
  # -- volumeMount used to expose istio certificates
  volumeMount:
    - name: istio-certs
      mountPath: /etc/istio-certs/
      readOnly: true

prometheus:
  # -- Annotations for Prometheus
  annotations: {}

  # -- APIServerConfig
  # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#apiserverconfig
  apiserverConfig: {}

  image:
    # -- Image of Prometheus to deploy.
    repository: quay.io/prometheus/prometheus

  ## Service account for Prometheuses to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    # -- When `true`, create prometheus serviceAccount with `prometheus.serviceAccount.name`. If `prometheus.serviceAccount.name` is empty, use `.Chart.fullname` value.
    create: true
    # -- Name of the ServiceAccount to use to run the Prometheus Pods. If `prometheus.serviceAccount.create` is `false` and no name is defined, use `default`.
    name: ""
    # -- Annotations for Prometheus
    annotations: {}

  # -- External labels to add to any time series or alerts when communicating with external systems
  externalLabels: {}

  # -- Name of the external label used to denote replica name
  replicaExternalLabelName: ""

  # -- If true, the Operator won't add the external label used to denote replica name
  replicaExternalLabelNameClear: false

  # -- Name of the external label used to denote Prometheus instance name
  prometheusExternalLabelName: ""

  # -- If true, the Operator won't add the external label used to denote Prometheus instance name
  prometheusExternalLabelNameClear: false

  # -- Define which Nodes the Pods are scheduled on.
  # ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  # -- Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
  # The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
  # reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
  # with the new list of secrets.
  secrets: []

  # -- ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
  # The ConfigMaps are mounted into /etc/prometheus/configmaps/.
  configMaps: []

  # -- Interval between consecutive evaluations.
  evaluationInterval: ""


  # -- EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
  # This is disabled by default.
  # ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis
  enableAdminAPI: false

  # -- External URL at which Prometheus will be reachable.
  externalUrl: ""

  # -- How long to retain metrics
  retention: 10d

  # -- Maximum size of metrics
  retentionSize: ""

  # -- Enable compression of the write-ahead log using Snappy.
  walCompression: false

  # -- If true, pass --storage.tsdb.max-block-duration=2h to prometheus
  disableCompaction: false

  # -- If true, the Operator won't process any Prometheus configuration changes
  paused: false

  # -- Number of Prometheus replicas desired
  replicas: 1

  # -- Log level for Prometheus be configured in
  logLevel: info

  # -- Log format for Prometheus be configured in
  logFormat: logfmt

  # -- Prefix used to register routes, overriding externalUrl route.
  # Useful for proxies that rewrite URLs.
  routePrefix: /

  # -- QuerySpec defines the query command line flags when starting Prometheus.
  # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#queryspec
  query: {}

  # -- Namespaces to be selected for PrometheusRules discovery.
  # If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.
  # See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#labelselector-v1-meta for usage
  ruleNamespaceSelector: {}

  # -- PrometheusRules to be selected for target discovery.
  # If {}, select all PrometheusRules.
  # See [values.yaml](./values.yaml) for example.
  ruleSelector: {}
  ## Example which select all prometheusrules resources
  ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
  # ruleSelector:
  #   matchExpressions:
  #     - key: prometheus
  #       operator: In
  #       values:
  #         - example-rules
  #         - example-rules-2
  #
  ## Example which select all prometheusrules resources with label "role" set to "example-rules"
  # ruleSelector:
  #   matchLabels:
  #     role: example-rules

  # -- ServiceMonitors to be selected for target discovery.
  # If {}, select all ServiceMonitors
  # See [values.yaml](./values.yaml) for example.
  serviceMonitorSelector: {}
  ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
  # serviceMonitorSelector:
  #   matchLabels:
  #     prometheus: somelabel

  # -- Namespaces to be selected for ServiceMonitor discovery.
  # See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#labelselector-v1-meta for usage
  serviceMonitorNamespaceSelector: {}

  # -- PodMonitors to be selected for target discovery.
  # If {}, select all PodMonitors
  # See [values.yaml](./values.yaml) for example.
  podMonitorSelector: {}
  ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
  # podMonitorSelector:
  #   matchLabels:
  #     prometheus: somelabel

  # -- Namespaces to be selected for PodMonitor discovery.
  # See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#labelselector-v1-meta for usage
  podMonitorNamespaceSelector: {}

  # -- The remote_read spec configuration for Prometheus.
  # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotereadspec
  remoteRead: []
  # - url: http://remote1/read

  # -- The remote_write spec configuration for Prometheus.
  # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
  remoteWrite: []
  # - url: http://remote1/push

  # -- Add resources limits and request to prometheus container.
  resources:
    limits:
      cpu: 1000m
      memory: 1000Mi
    requests:
      cpu: 500m
      memory: 700Mi


  # -- Interval between consecutive scrapes.
  scrapeInterval: ""

  # -- Configure pod disruption budgets for Prometheus
  # ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  # This configuration is immutable once created and will require the PDB to be deleted to be changed
  # https://github.com/kubernetes/kubernetes/issues/45398
  #
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  securityContext:
    # -- Indicates that the container must run as a non-root user
    runAsNonRoot: true
    # -- The UID to run the entrypoint of the container process
    runAsUser: 1000
    # -- The GID to run the entrypoint of the container process
    runAsGroup: 1000
    # --
    fsGroup: 1000

  # -- Standard object’s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
  # Metadata Labels and Annotations gets propagated to the prometheus pods.
  podMetadata:
    # -- Labels to add add on prometheus pod
    labels: {}
    # -- Annotations to add add on prometheus pod
    annotations: {}

  # -- Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
  # The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
  # The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
  # The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
  podAntiAffinity: ""

  # -- If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
  # This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
  podAntiAffinityTopologyKey: kubernetes.io/hostname

  # -- Assign custom affinity rules to the prometheus instance
  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  affinity: {}
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: kubernetes.io/e2e-az-name
  #         operator: In
  #         values:
  #         - e2e-az1
  #         - e2e-az2

  # -- Tolerations for use with node taints
  # ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []
  #  - key: "key"
  #    operator: "Equal"
  #    value: "value"
  #    effect: "NoSchedule"

  # -- AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
  # are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
  # as specified in the official Prometheus documentation:
  # https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
  # appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
  # to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
  # scrape configs are going to break Prometheus after the upgrade.
  #
  # The scrape configuraiton example below will find master nodes, provided they have the name .*mst.*, relabel the
  # port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
  additionalScrapeConfigs: []
  # - job_name: kube-etcd
  #   kubernetes_sd_configs:
  #     - role: node
  #   scheme: https
  #   tls_config:
  #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
  #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
  #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
  #   relabel_configs:
  #   - action: labelmap
  #     regex: __meta_kubernetes_node_label_(.+)
  #   - source_labels: [__address__]
  #     action: replace
  #     targetLabel: __address__
  #     regex: ([^:;]+):(\d+)
  #     replacement: ${1}:2379
  #   - source_labels: [__meta_kubernetes_node_name]
  #     action: keep
  #     regex: .*mst.*
  #   - source_labels: [__meta_kubernetes_node_name]
  #     action: replace
  #     targetLabel: node
  #     regex: (.*)
  #     replacement: ${1}
  #   metric_relabel_configs:
  #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
  #     action: labeldrop

  # -- If additional scrape configurations are already deployed in a single secret file you can use this section.
  # Expected values are the secret name and key
  # Cannot be used with additionalScrapeConfigs
  additionalScrapeConfigsSecret: {}
    # enabled: false
    # name:
    # key:

  # -- AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
  # in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
  # AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
  # As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
  # feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
  # notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
  additionalAlertManagerConfigs: []
  # - consul_sd_configs:
  #   - server: consul.dev.test:8500
  #     scheme: http
  #     datacenter: dev
  #     tag_separator: ','
  #     services:
  #       - metrics-prometheus-alertmanager

  # -- AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
  # to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
  # official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
  # As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
  # possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
  # configs are going to break Prometheus after the upgrade.
  additionalAlertRelabelConfigs: []
  # - separator: ;
  #   regex: prometheus_replica
  #   replacement: $1
  #   action: labeldrop

  # -- Priority class assigned to the Pods
  priorityClassName: ""

  # -- Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.
  # if using proxy extraContainer, update targetPort with proxy container port
  containers: []

  # --  InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
  # (permissions, dir tree) on mounted volumes before starting prometheus
  initContainers: []

  # -- PortName to use for Prometheus.
  portName: "http-web"

  # -- Prometheus StorageSpec for persistent data
  # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md
  storageSpec: {}
  #  volumeClaimTemplate:
  #    spec:
  #      storageClassName: standard
  #      accessModes: ["ReadWriteOnce"]
  #      resources:
  #        requests:
  #          storage: 50Gi
  #    selector: {}

  # -- Additional volumes on the output StatefulSet definition.
  volumes: []
  # -- Additional VolumeMounts on the output StatefulSet definition.
  volumeMounts: []

  # -- EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created. The label value will always be the namespace of the object that is being created.
  enforcedNamespaceLabel: ""


## Configuration for Prometheus service
##
service:
  # -- Map of labels to apply to the service
  labels: {}
  # -- Map of annotations to apply to the service
  annotations: {}

  # -- Cluster IP
  # Only use if service.type is "ClusterIP"
  clusterIP: ""

  # -- Port for Prometheus Service to listen on
  port: 9090

  # -- To be used with a proxy extraContainer port
  targetPort: 9090

  # -- List of IP addresses at which the Prometheus server service is available
  # Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  externalIPs: []

  # -- Port to expose on each node
  # Only used if service.type is 'NodePort'
  nodePort: 30090

  # -- Loadbalancer IP
  # Only use if service.type is "loadbalancer"
  loadBalancerIP: ""
  # service.loadSourceRanges -- Only use if service.type is "loadbalancer"
  loadBalancerSourceRanges: []

  # -- Service type
  type: ClusterIP
  # service.sessionAffinity --
  sessionAffinity: ""


ingress:
  # -- Enables ingress for prometheus
  enabled: false
  # -- Map of labels to apply to the ingress
  labels: {}
  #  -- Map of default annotations to apply to the ingress
  annotations:
    kubernetes.io/ingress.allow-http: "false"
  acme:
    # -- Manage certificates with ACME protocol
    enabled: true
    # -- Annotations to add when `ingress.acme.enabled` is true
    annotations:
      - 'kubernetes.io/tls-acme: "true"'

  # -- Map of annotations to add  to th ingress
  extraAnnotations: {}
  # -- FQDN of the prometheus
  host: ""
  #  -- Path of the incoming request (/* or / if used with nginx)
  path: "/*"
  tls:
    # -- Name of the secret containing the certificates
    # @default -- Generated name based on chart release full name
    secretName: ""


serviceMonitor:
  # -- If `true` Self monitor prometheus with ServiceMonitor
  enabled: false

  # -- Scrape interval. If not set, the Prometheus default scrape interval is used. dd
  interval: ""

  # -- HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
  scheme: ""

  # -- TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
  # Of type: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
  tlsConfig: {}

  # --
  bearerTokenFile:

  # -- metric relabel configs to apply to samples before ingestion.
  metricRelabelings: []
  # - action: keep
  #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
  #   sourceLabels: [__name__]

  # -- Relabel configs to apply to samples before ingestion.
  relabelings: []
  # - sourceLabels: [__meta_kubernetes_pod_node_name]
  #   separator: ;
  #   regex: ^(.*)$
  #   targetLabel: nodename
  #   replacement: $1
  #   action: replace

  # -- Map of annotations to apply to the ServiceMonitor
  annotations: {}


oidc:
  global: {}
  # To help compatibility with other charts which use global.imagePullSecrets.
  # global:
  #   imagePullSecrets:
  #   - name: pullSecret1
  #   - name: pullSecret2

  ## Override the deployment namespace
  ##
  namespaceOverride: ""

  # Force the target Kubernetes version (it uses Helm `.Capabilities` if not set).
  # This is especially useful for `helm template` as capabilities are always empty
  # due to the fact that it doesn't query an actual cluster
  kubeVersion:

  # Oauth client configuration specifics
  config:
    # Add config annotations
    annotations: {}
    # OAuth client ID
    clientID: "XXXXXXX"
    # OAuth client secret
    clientSecret: "XXXXXXXX"
    # Create a new secret with the following command
    # openssl rand -base64 32 | head -c 32 | base64
    # Use an existing secret for OAuth2 credentials (see secret.yaml for required fields)
    # Example:
    # existingSecret: secret
    cookieSecret: "XXXXXXXXXXXXXXXX"
    # The name of the cookie that oauth2-proxy will create
    # If left empty, it will default to the release name
    cookieName: ""
    google: {}
      # adminEmail: xxxx
      # useApplicationDefaultCredentials: true
      # targetPrincipal: xxxx
      # serviceAccountJson: xxxx
      # Alternatively, use an existing secret (see google-secret.yaml for required fields)
      # Example:
      # existingSecret: google-secret
      # groups: []
      # Example:
      #  - group1@example.com
      #  - group2@example.com
    # Default configuration, to be overridden
    configFile: |-
      email_domains = [ "*" ]
      upstreams = [ "file:///dev/null" ]
    # Custom configuration file: oauth2_proxy.cfg
    # configFile: |-
    #   pass_basic_auth = false
    #   pass_access_token = true
    # Use an existing config map (see configmap.yaml for required fields)
    # Example:
    # existingConfig: config

  alphaConfig:
    enabled: false
    # Add config annotations
    annotations: {}
    # Arbitrary configuration data to append to the server section
    serverConfigData: {}
    # Arbitrary configuration data to append to the metrics section
    metricsConfigData: {}
    # Arbitrary configuration data to append
    configData: {}
    # Arbitrary configuration to append
    # This is treated as a Go template and rendered with the root context
    configFile: ""
    # Use an existing config map (see secret-alpha.yaml for required fields)
    existingConfig: ~
    # Use an existing secret
    existingSecret: ~

  image:
    repository: "quay.io/oauth2-proxy/oauth2-proxy"
    # appVersion is used by default
    tag: ""
    pullPolicy: "IfNotPresent"
    command: []

  # Optionally specify an array of imagePullSecrets.
  # Secrets must be manually created in the namespace.
  # ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  imagePullSecrets: []
    # - name: myRegistryKeySecretName

  # Set a custom containerPort if required.
  # This will default to 4180 if this value is not set and the httpScheme set to http
  # This will default to 4443 if this value is not set and the httpScheme set to https
  # containerPort: 4180

  extraArgs: {}
  extraEnv: []

  envFrom: []
  # Load environment variables from a ConfigMap(s) and/or Secret(s)
  # that already exists (created and managed by you).
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables
  #
  # PS: Changes in these ConfigMaps or Secrets will not be automatically
  #     detected and you must manually restart the relevant Pods after changes.
  #
  #  - configMapRef:
  #      name: special-config
  #  - secretRef:
  #      name: special-config-secret

  # -- Custom labels to add into metadata
  customLabels: {}

  # To authorize individual email addresses
  # That is part of extraArgs but since this needs special treatment we need to do a separate section
  authenticatedEmailsFile:
    enabled: false
    # Defines how the email addresses file will be projected, via a configmap or secret
    persistence: configmap
    # template is the name of the configmap what contains the email user list but has been configured without this chart.
    # It's a simpler way to maintain only one configmap (user list) instead changing it for each oauth2-proxy service.
    # Be aware the value name in the extern config map in data needs to be named to "restricted_user_access" or to the
    # provided value in restrictedUserAccessKey field.
    template: ""
    # The configmap/secret key under which the list of email access is stored
    # Defaults to "restricted_user_access" if not filled-in, but can be overridden to allow flexibility
    restrictedUserAccessKey: ""
    # One email per line
    # example:
    # restricted_access: |-
    #   name1@domain
    #   name2@domain
    # If you override the config with restricted_access it will configure a user list within this chart what takes care of the
    # config map resource.
    restricted_access: ""
    annotations: {}
    # helm.sh/resource-policy: keep

  service:
    type: ClusterIP
    # when service.type is ClusterIP ...
    # clusterIP: 192.0.2.20
    # when service.type is LoadBalancer ...
    # loadBalancerIP: 198.51.100.40
    # loadBalancerSourceRanges: 203.0.113.0/24
    # when service.type is NodePort ...
    # nodePort: 80
    portNumber: 80
    # Protocol set on the service
    appProtocol: http
    annotations: {}
    # foo.io/bar: "true"
    # configure externalTrafficPolicy
    externalTrafficPolicy: ""
    # configure internalTrafficPolicy
    internalTrafficPolicy: ""
    # configure service target port
    targetPort: ""

  ## Create or use ServiceAccount
  serviceAccount:
    ## Specifies whether a ServiceAccount should be created
    enabled: true
    ## The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the fullname template
    name:
    automountServiceAccountToken: true
    annotations: {}

  ingress:
    enabled: false
    # className: nginx
    path: /
    # Only used if API capabilities (networking.k8s.io/v1) allow it
    pathType: ImplementationSpecific
    # Used to create an Ingress record.
    # hosts:
      # - chart-example.local
    # Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    # Warning! The configuration is dependant on your current k8s API version capabilities (networking.k8s.io/v1)
    # extraPaths:
    # - path: /*
    #   pathType: ImplementationSpecific
    #   backend:
    #     service:
    #       name: ssl-redirect
    #       port:
    #         name: use-annotation
    labels: {}
    # annotations:
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: "true"
    # tls:
      # Secrets must be manually created in the namespace.
      # - secretName: chart-example-tls
      #   hosts:
      #     - chart-example.local

  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 300Mi
    # requests:
    #   cpu: 100m
    #   memory: 300Mi

  extraVolumes: []
    # - name: ca-bundle-cert
    #   secret:
    #     secretName: <secret-name>

  extraVolumeMounts: []
    # - mountPath: /etc/ssl/certs/
    #   name: ca-bundle-cert

  # Additional containers to be added to the pod.
  extraContainers: []
    #  - name: my-sidecar
    #    image: nginx:latest

  # Additional Init containers to be added to the pod.
  extraInitContainers: []
    #  - name: wait-for-idp
    #    image: my-idp-wait:latest
    #    command:
    #    - sh
    #    - -c
    #    - wait-for-idp.sh

  priorityClassName: ""

  # hostAliases is a list of aliases to be added to /etc/hosts for network name resolution
  hostAliases: []
  # - ip: "10.xxx.xxx.xxx"
  #   hostnames:
  #     - "auth.example.com"
  # - ip: 127.0.0.1
  #   hostnames:
  #     - chart-example.local
  #     - example.local

  # [TopologySpreadConstraints](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/) configuration.
  # Ref: https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling
  # topologySpreadConstraints: []

  # Affinity for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  # affinity: {}

  # Tolerations for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # Node labels for pod assignment
  # Ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  # Whether to use secrets instead of environment values for setting up OAUTH2_PROXY variables
  proxyVarsAsSecrets: true

  # Configure Kubernetes liveness and readiness probes.
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
  # Disable both when deploying with Istio 1.0 mTLS. https://istio.io/help/faq/security/#k8s-health-checks
  livenessProbe:
    enabled: true
    initialDelaySeconds: 0
    timeoutSeconds: 1

  readinessProbe:
    enabled: true
    initialDelaySeconds: 0
    timeoutSeconds: 5
    periodSeconds: 10
    successThreshold: 1

  # Configure Kubernetes security context for container
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  securityContext:
    enabled: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 2000
    runAsGroup: 2000
    seccompProfile:
      type: RuntimeDefault

  deploymentAnnotations: {}
  podAnnotations: {}
  podLabels: {}
  replicaCount: 1
  revisionHistoryLimit: 10
  strategy: {}
  enableServiceLinks: true

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  ## Horizontal Pod Autoscaling
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
  #  targetMemoryUtilizationPercentage: 80
    annotations: {}

  # Configure Kubernetes security context for pod
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  podSecurityContext: {}

  # whether to use http or https
  httpScheme: http

  initContainers:
    # if the redis sub-chart is enabled, wait for it to be ready
    # before starting the proxy
    # creates a role binding to get, list, watch, the redis master pod
    # if service account is enabled
    waitForRedis:
      enabled: true
      image:
        repository: "alpine"
        tag: "latest"
        pullPolicy: "IfNotPresent"
      # uses the kubernetes version of the cluster
      # the chart is deployed on, if not set
      kubectlVersion: ""
      securityContext:
        enabled: true
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      timeout: 180
      resources: {}
        # limits:
        #   cpu: 100m
        #   memory: 300Mi
        # requests:
        #   cpu: 100m
        #   memory: 300Mi

  # Additionally authenticate against a htpasswd file. Entries must be created with "htpasswd -B" for bcrypt encryption.
  # Alternatively supply an existing secret which contains the required information.
  htpasswdFile:
    enabled: false
    existingSecret: ""
    entries: []
    # One row for each user
    # example:
    # entries:
    #  - testuser:$2y$05$gY6dgXqjuzFhwdhsiFe7seM9q9Tile4Y3E.CBpAZJffkeiLaC21Gy

  # Configure the session storage type, between cookie and redis
  sessionStorage:
    # Can be one of the supported session storage cookie|redis
    type: cookie
    redis:
      # Name of the Kubernetes secret containing the redis & redis sentinel password values (see also `sessionStorage.redis.passwordKey`)
      existingSecret: ""
      # Redis password value. Applicable for all Redis configurations. Taken from redis subchart secret if not set. `sessionStorage.redis.existingSecret` takes precedence
      password: ""
      # Key of the Kubernetes secret data containing the redis password value. If you use the redis sub chart, make sure
      # this password matches the one used in redis.global.redis.password (see below).
      passwordKey: "redis-password"
      # Can be one of standalone|cluster|sentinel
      clientType: "standalone"
      standalone:
        # URL of redis standalone server for redis session storage (e.g. `redis://HOST[:PORT]`). Automatically generated if not set
        connectionUrl: ""
      cluster:
        # List of Redis cluster connection URLs. Array or single string allowed.
        connectionUrls: []
        # - "redis://127.0.0.1:8000"
        # - "redis://127.0.0.1:8001"
      sentinel:
        # Name of the Kubernetes secret containing the redis sentinel password value (see also `sessionStorage.redis.sentinel.passwordKey`). Default: `sessionStorage.redis.existingSecret`
        existingSecret: ""
        # Redis sentinel password. Used only for sentinel connection; any redis node passwords need to use `sessionStorage.redis.password`
        password: ""
        # Key of the Kubernetes secret data containing the redis sentinel password value
        passwordKey: "redis-sentinel-password"
        # Redis sentinel master name
        masterName: ""
        # List of Redis cluster connection URLs. Array or single string allowed.
        connectionUrls: []
        # - "redis://127.0.0.1:8000"
        # - "redis://127.0.0.1:8001"

  # Enables and configure the automatic deployment of the redis subchart
  redis:
    # provision an instance of the redis sub-chart
    enabled: false
    # Redis specific helm chart settings, please see:
    # https://github.com/bitnami/charts/tree/master/bitnami/redis#parameters
    # global:
    #   redis:
    #     password: yourpassword
    # If you install Redis using this sub chart, make sure that the password of the sub chart matches the password
    # you set in sessionStorage.redis.password (see above).
    # redisPort: 6379
    # architecture: standalone

  # Enables apiVersion deprecation checks
  checkDeprecation: true

  # Allows graceful shutdown
  # terminationGracePeriodSeconds: 65
  # lifecycle:
  #   preStop:
  #     exec:
  #       command: [ "sh", "-c", "sleep 60" ]

  metrics:
    # Enable Prometheus metrics endpoint
    enabled: true
    # Serve Prometheus metrics on this port
    port: 44180
    # when service.type is NodePort ...
    # nodePort: 44180
    # Protocol set on the service for the metrics port
    service:
      appProtocol: http
    serviceMonitor:
      # Enable Prometheus Operator ServiceMonitor
      enabled: false
      # Define the namespace where to deploy the ServiceMonitor resource
      namespace: ""
      # Prometheus Instance definition
      prometheusInstance: default
      # Prometheus scrape interval
      interval: 60s
      # Prometheus scrape timeout
      scrapeTimeout: 30s
      # Add custom labels to the ServiceMonitor resource
      labels: {}

      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
      tlsConfig: {}

      ## bearerTokenFile: Path to bearer token file.
      bearerTokenFile: ""

      ## Used to pass annotations that are used by the Prometheus installed in your cluster to select Service Monitors to work with
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      annotations: {}

      ## Metric relabel configs to apply to samples before ingestion.
      ## [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## Relabel configs to apply to samples before ingestion.
      ## [Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config)
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

  # Extra K8s manifests to deploy
  extraObjects: []
    # - apiVersion: secrets-store.csi.x-k8s.io/v1
    #   kind: SecretProviderClass
    #   metadata:
    #     name: oauth2-proxy-secrets-store
    #   spec:
    #     provider: aws
    #     parameters:
    #       objects: |
    #         - objectName: "oauth2-proxy"
    #           objectType: "secretsmanager"
    #           jmesPath:
    #               - path: "client_id"
    #                 objectAlias: "client-id"
    #               - path: "client_secret"
    #                 objectAlias: "client-secret"
    #               - path: "cookie_secret"
    #                 objectAlias: "cookie-secret"
    #     secretObjects:
    #     - data:
    #       - key: client-id
    #         objectName: client-id
    #         - key: client-secret
    #           objectName: client-secret
    #         - key: cookie-secret
    #         objectName: cookie-secret
    #       secretName: oauth2-proxy-secrets-store
    #       type: Opaque
rules:
  # -- If `true`, create prometheus rules to monitor prometheus instance when ServiceMonitor is enabled
  enabled: true
  # --  Map of labels to apply to the prometheus rules.
  # This labels are used to define `prometheus.ruleSelector`
  # @default -- `{prometheus: "release chart fullname"}`
  labels: {}
  # -- Map of annotations to apply to the prometheus rules created
  annotations: {}
